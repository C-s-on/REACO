{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:12:20.718932Z",
     "iopub.status.busy": "2024-06-08T05:12:20.718585Z",
     "iopub.status.idle": "2024-06-08T05:12:20.725927Z",
     "shell.execute_reply": "2024-06-08T05:12:20.724917Z",
     "shell.execute_reply.started": "2024-06-08T05:12:20.718908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if a GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:12:33.529812Z",
     "iopub.status.busy": "2024-06-08T05:12:33.52915Z",
     "iopub.status.idle": "2024-06-08T05:12:33.536019Z",
     "shell.execute_reply": "2024-06-08T05:12:33.534764Z",
     "shell.execute_reply.started": "2024-06-08T05:12:33.529778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D, BatchNormalization, Dropout, Layer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:12:47.071361Z",
     "iopub.status.busy": "2024-06-08T05:12:47.070999Z",
     "iopub.status.idle": "2024-06-08T05:12:47.076203Z",
     "shell.execute_reply": "2024-06-08T05:12:47.075233Z",
     "shell.execute_reply.started": "2024-06-08T05:12:47.071336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:13:05.971112Z",
     "iopub.status.busy": "2024-06-08T05:13:05.970252Z",
     "iopub.status.idle": "2024-06-08T05:13:10.288935Z",
     "shell.execute_reply": "2024-06-08T05:13:10.28797Z",
     "shell.execute_reply.started": "2024-06-08T05:13:05.97107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "BASE_PATH = r'/kaggle/input/faces-935'\n",
    "SUB_FOLDERS = os.listdir(BASE_PATH)\n",
    "\n",
    "# Image preprocessing\n",
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=(200, 200))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return img_array\n",
    "\n",
    "# Data generator function to create triplets\n",
    "def create_triplets(sub_folders):\n",
    "    triplets = []\n",
    "\n",
    "    for anchor_folder in sub_folders:\n",
    "        anchor_images = os.listdir(os.path.join(BASE_PATH, anchor_folder))\n",
    "        for anchor_img in anchor_images:\n",
    "            anchor_img_path = os.path.join(BASE_PATH, anchor_folder, anchor_img)\n",
    "            \n",
    "            # Choose a random negative image from a different sub-folder\n",
    "            negative_folder = random.choice(sub_folders)\n",
    "            while negative_folder == anchor_folder:\n",
    "                negative_folder = random.choice(sub_folders)\n",
    "            negative_images = os.listdir(os.path.join(BASE_PATH, negative_folder))\n",
    "            negative_img = random.choice(negative_images)\n",
    "            negative_img_path = os.path.join(BASE_PATH, negative_folder, negative_img)\n",
    "            \n",
    "            # Choose a random positive image from the same anchor folder\n",
    "            positive_img = random.choice(anchor_images)\n",
    "            while positive_img == anchor_img:\n",
    "                positive_img = random.choice(anchor_images)\n",
    "            positive_img_path = os.path.join(BASE_PATH, anchor_folder, positive_img)\n",
    "            \n",
    "            triplets.append([anchor_img_path, positive_img_path, negative_img_path])\n",
    "    \n",
    "    print(f\"Created {len(triplets)} triplets\")\n",
    "    return np.array(triplets)\n",
    "\n",
    "triplets = create_triplets(SUB_FOLDERS)\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "def triplet_generator(triplets):\n",
    "    for triplet in triplets:\n",
    "        anchor_img = preprocess_image(triplet[0])\n",
    "        positive_img = preprocess_image(triplet[1])\n",
    "        negative_img = preprocess_image(triplet[2])\n",
    "        yield ({\n",
    "            'anchor': anchor_img,\n",
    "            'positive': positive_img,\n",
    "            'negative': negative_img\n",
    "        }, [0.0])  # Dummy label\n",
    "\n",
    "# Reduced batch size to handle limited data\n",
    "batch_size = 32\n",
    "\n",
    "triplet_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: triplet_generator(triplets), \n",
    "    output_signature=(\n",
    "        {\n",
    "            'anchor': tf.TensorSpec(shape=(200, 200, 3), dtype=tf.float32),\n",
    "            'positive': tf.TensorSpec(shape=(200, 200, 3), dtype=tf.float32),\n",
    "            'negative': tf.TensorSpec(shape=(200, 200, 3), dtype=tf.float32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(1,), dtype=tf.float32)  # Dummy label\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cache images to improve performance\n",
    "triplet_dataset = triplet_dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:13:47.982225Z",
     "iopub.status.busy": "2024-06-08T05:13:47.981382Z",
     "iopub.status.idle": "2024-06-08T05:13:49.179638Z",
     "shell.execute_reply": "2024-06-08T05:13:49.17865Z",
     "shell.execute_reply.started": "2024-06-08T05:13:47.982194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Repeat dataset to ensure it does not run out of data\n",
    "triplet_dataset = triplet_dataset.shuffle(buffer_size=512).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "total_size = len(triplets)\n",
    "train_size = int(0.8 * total_size)  # Adjusted split ratio for train/test\n",
    "test_size = total_size - train_size\n",
    "steps_per_epoch = train_size // batch_size \n",
    "validation_steps = test_size // batch_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = triplet_dataset.take(train_size).repeat()\n",
    "test_dataset = triplet_dataset.skip(train_size).take(test_size).repeat()\n",
    "\n",
    "# Build Embedding Model with ResNet50\n",
    "def make_embedding():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        # Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),  # Larger Dense layer\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),  # Adding dropout to prevent overfitting\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),   # Last Dense layer\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5)  # Adding dropout to prevent overfitting\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "embedding = make_embedding()\n",
    "\n",
    "# Siamese Network Model with Triplet Loss\n",
    "input_anchor = Input(shape=(200, 200, 3), name='anchor')\n",
    "input_positive = Input(shape=(200, 200, 3), name='positive')\n",
    "input_negative = Input(shape=(200, 200, 3), name='negative')\n",
    "\n",
    "embedding_anchor = embedding(input_anchor)\n",
    "embedding_positive = embedding(input_positive)\n",
    "embedding_negative = embedding(input_negative)\n",
    "\n",
    "# Custom layer for concatenation and distance calculation\n",
    "class TripletConcatenateLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        return tf.concat([anchor, positive, negative], axis=1)\n",
    "\n",
    "concatenated_embeddings = TripletConcatenateLayer()([embedding_anchor, embedding_positive, embedding_negative])\n",
    "\n",
    "# Triplet Loss function\n",
    "def triplet_loss(margin=0.5):\n",
    "    def loss(y_true, y_pred):\n",
    "        anchor, positive, negative = tf.split(y_pred, num_or_size_splits=3, axis=1)\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        basic_loss = pos_dist - neg_dist + margin\n",
    "        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))\n",
    "        return loss\n",
    "    return loss\n",
    "\n",
    "# Combine the embeddings into a model\n",
    "siamese_model = Model(inputs=[input_anchor, input_positive, input_negative], \n",
    "                      outputs=concatenated_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:14:11.663176Z",
     "iopub.status.busy": "2024-06-08T05:14:11.66278Z",
     "iopub.status.idle": "2024-06-08T06:02:03.036168Z",
     "shell.execute_reply": "2024-06-08T06:02:03.035291Z",
     "shell.execute_reply.started": "2024-06-08T05:14:11.663119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model with the triplet loss and optimizer improvements\n",
    "print(\"Compiling the model...\")\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0), loss=triplet_loss())\n",
    "print(\"Model compiled successfully\")\n",
    "\n",
    "# Custom callback to save the best embedding model\n",
    "class SaveBestEmbeddingModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, embedding_model, save_path, monitor='val_loss', mode='min'):\n",
    "        super(SaveBestEmbeddingModel, self).__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.save_path = save_path\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None:\n",
    "            if (self.mode == 'min' and current < self.best) or (self.mode == 'max' and current > self.best):\n",
    "                self.best = current\n",
    "                self.embedding_model.save(self.save_path)\n",
    "                print(f\"\\nSaved best embedding model to {self.save_path} (epoch {epoch + 1}, {self.monitor}: {current})\")\n",
    "\n",
    "# Train the model with learning rate scheduling\n",
    "print(\"Starting model training...\")\n",
    "EPOCHS = 100  # Set appropriate epochs for better training\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Instantiate the custom callback\n",
    "save_best_embedding_model = SaveBestEmbeddingModel(embedding, 'best_embedding_model.h5', monitor='val_loss', mode='min')\n",
    "\n",
    "history = siamese_model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, \n",
    "                            validation_data=test_dataset, validation_steps=validation_steps, \n",
    "                            callbacks=[lr_scheduler, early_stopping, save_best_embedding_model])\n",
    "print(\"Model training completed\")\n",
    "\n",
    "# Remove the existing model file if it exists\n",
    "model_path = '/kaggle/working/siamese_model.keras'\n",
    "if os.path.exists(model_path):\n",
    "    os.remove(model_path)\n",
    "\n",
    "# Save model in Keras format\n",
    "siamese_model.save(model_path)\n",
    "print(\"Model Saved\")\n",
    "\n",
    "# Save the full Siamese model in .h5 format\n",
    "model_path_h5 = '/kaggle/working/siamese_model.h5'\n",
    "siamese_model.save(model_path_h5)\n",
    "print(\"Full Siamese model saved as .h5 file\")\n",
    "\n",
    "# Save just the embedding model\n",
    "embedding_model_path = '/kaggle/working/embedding_model.h5'\n",
    "embedding.save(embedding_model_path)\n",
    "print(\"Embedding model saved as .h5 file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T06:05:03.960367Z",
     "iopub.status.busy": "2024-06-08T06:05:03.959598Z",
     "iopub.status.idle": "2024-06-08T06:05:04.290198Z",
     "shell.execute_reply": "2024-06-08T06:05:04.289231Z",
     "shell.execute_reply.started": "2024-06-08T06:05:03.960335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(history)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5171957,
     "sourceId": 8636815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
